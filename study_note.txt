Example C++ component contributions: https://github.com/IBM/aihwkit/pull/356

Implements a new device model class (SelfDefineDevice) where user can define up/down pulse and weights.
From my point of view we could merge it. Though before that we could also change/ add the following:

1.Add documentation of how to fit an experimental device
2.Add the device fit function from the example into the main code e.g. under aihwkit/utils/fitting.py
3.The fitting could be improved for odd bins, since the update step is not quite correct then (as the bin size is estimated for the middle of the bin)

change log:
* Out scaling factors can be learnt even if weight scaling omega was set to 0. (\#360)
* Reverse up / down option for ``LinearStepDevice``. (\#361)
* Generic Analog RNN classes (LSTM, RNN, GRU) uni or bidirectional. (\#358)
* Added new ``PiecewiseStepDevice`` that can be conviniently used for experimental
  device data fitting. (\#356)

### Fixed

* Legacy checkpoint load with alpha scaling. (\#360)
* Re-application of weight scaling omega when loading checkpoints. (\#360)
* Write noise was not correctly applied for CUDA if ``dw_min_std=0``. (\#356)

class:`~aihwkit.simulator.configs.devices.PiecewiseStepDevice`      user defined device

the sorce code:

class PiecewiseStepDevice(PulsedDevice):
    r"""Self defined device characteristics.
    This model is derived from ``PulsedDevice`` and uses all its
    parameters. ''PiecewiseStepDevice'' implements a new functionality
    where the device characteristics are defined by the user.
    Up and down pulse values are stored in selfdefine.csv and the
    number of points can be decreased to 2 and increased indefinitely.
    """
    bindings_class: ClassVar[Type] = devices.PiecewiseStepResistiveDeviceParameter

    piecewise_up: List[float] = field(default_factory=lambda: [1])
    r"""Array of values that characterize the update steps in upwards direction.
    The values are equally spaced in ``w_min`` and `w_max``, where the
    first and the last value is set at the boundary. The update will
    be computed by linear interpolation of the adjacent values,
    depending on where the weight is currently within the range.
    The values are given as relative numbers: the final update size
    will be computed by multiplying the value with the current
    ``dw_min`` of the device.
    E.g.  ``[1.5, 1, 1.5]`` and ``dw_min=0.1`` means that the update
    (in up direction) is ``dw_min`` around zero weight value and
    linearly increasing to ``1.5 * dw_min`` for larger or smaller
    weight values.
    """

    piecewise_down: List[float] = field(default_factory=lambda: [1])
    r"""Array of values that characterize the update steps in downwards direction.
    Analogous to ``piecewise_up`` but for the downwards direction.
    """

    write_noise_std: float = 0.0
    r"""Whether to use update write noise.
    Whether to use update write noise that is added to the updated
    devices weight, while the update is done on a hidden persistent weight. The
    update write noise is then sampled a new when the device is touched
    again.
    Thus it is:
    .. math::
        w_\text{apparent}{ij} = w_{ij} + \sigma_\text{write_noise}\xi
    and the update is done on :math:`w_{ij}` but the forward sees the
    :math:`w_\text{apparent}`.
    """

    code in c++:
    class PyPiecewiseStepParam : public PiecewiseStepParam {
  public:
    std::string getName() const override {
      PYBIND11_OVERLOAD(std::string, PiecewiseStepParam, getName, );
    }
    PiecewiseStepParam *clone() const override {
      PYBIND11_OVERLOAD(PiecewiseStepParam *, PiecewiseStepParam, clone, );
    }
    RPU::DeviceUpdateType implements() const override {
      PYBIND11_OVERLOAD(RPU::DeviceUpdateType, PiecewiseStepParam, implements, );
    }
    RPU::PiecewiseStepRPUDevice<T> *
    createDevice(int x_size, int d_size, RPU::RealWorldRNG<T> *rng) override {
      PYBIND11_OVERLOAD(
          RPU::PiecewiseStepRPUDevice<T> *, PiecewiseStepParam, createDevice, x_size, d_size, rng);
    }
    T calcWeightGranularity() const override {
      PYBIND11_OVERLOAD(T, PiecewiseStepParam, calcWeightGranularity, );
    }
  };

  the binding part:
  py::class_<PiecewiseStepParam, PyPiecewiseStepParam, PulsedParam>(
      m, "PiecewiseStepResistiveDeviceParameter")
      .def(py::init<>())
      .def_readwrite("piecewise_up", &PiecewiseStepParam::piecewise_up_vec)
      .def_readwrite("piecewise_down", &PiecewiseStepParam::piecewise_down_vec)
      .def_readwrite("write_noise_std", &PiecewiseStepParam::write_noise_std)
      .def(
          "__str__",
          [](PiecewiseStepParam &self) {
            std::stringstream ss;
            self.printToStream(ss);
            return ss.str();
          })
      .def(
          "calc_weight_granularity", &PiecewiseStepParam::calcWeightGranularity,
          R"pbdoc(
        Calculates the granularity of the weights (typically ``dw_min``)
        Returns:
           float: weight granularity
        )pbdoc");


the second example:

Created a wrapper AnalogRNN class that accepts different cells (LSTM, GRU, VanillaRNN). Also has the option of creating a bidirectional layer.

3 files were created: cells.py, layers.py, and rnn.py. All of these are in a rnn folder in nn/modules.

* Set weights can be used to re-apply the weight scaling omega. (\#360)
* Out scaling factors can be learnt even if weight scaling omega was set to 0. (\#360)
* Reverse up / down option for ``LinearStepDevice``. (\#361)
* Generic Analog RNN classes (LSTM, RNN, GRU) uni or bidirectional. (\#358)

### Fixed

* Legacy checkpoint load with alpha scaling. (\#360)
* Re-application of weight scaling omega when loading checkpoints. (\#360) 
* Re-application of weight scaling omega when loading checkpoints. (\#360)

### Changed

@@ -82,7 +83,7 @@ The format is based on [Keep a Changelog], and this project adheres to
  types if set larger than zero. (\#318)
* The use of generators for analog tiles of an ``AnalogModuleBase``. (\#320)
* Digital bias is now accessable through ``MappingParameter``. (\#331)
* The aihwkit documentation. New content around analog ai concepts, training presets, analog ai 
* The aihwkit documentation. New content around analog ai concepts, training presets, analog ai
  optimizers, new references, and examples. (\#348)
* The `weight_scaling_omega` can now be defined in the `rpu_config.mapping`. (\#353)

cell:
from torch import Tensor, sigmoid, tanh, zeros

from aihwkit.nn import AnalogLinear, AnalogSequential
from aihwkit.simulator.configs import InferenceRPUConfig
from aihwkit.nn.modules.base import RPUConfigAlias

LSTMState = namedtuple('LSTMState', ['hx', 'cx'])


class AnalogVanillaRNNCell(AnalogSequential):
    """Analog Vanilla RNN Cell.
    Args:
        input_size: in_features size for W_ih matrix
        hidden_size: in_features and out_features size for W_hh matrix
        bias: whether to use a bias row on the analog tile or not
        rpu_config: configuration for an analog resistive processing unit
        realistic_read_write: whether to enable realistic read/write
            for setting initial weights and read out of weights
    """
    # pylint: disable=abstract-method
    def __init__(
            self,
            input_size: int,
            hidden_size: int,
            bias: bool,
            rpu_config: Optional[RPUConfigAlias] = None,
            realistic_read_write: bool = False,
    ):
        super().__init__()

        # Default to InferenceRPUConfig
        if not rpu_config:
            rpu_config = InferenceRPUConfig()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.weight_ih = AnalogLinear(input_size, hidden_size, bias=bias,
                                      rpu_config=rpu_config,
                                      realistic_read_write=realistic_read_write)
        self.weight_hh = AnalogLinear(hidden_size, hidden_size, bias=bias,
                                      rpu_config=rpu_config,
                                      realistic_read_write=realistic_read_write)

    def get_zero_state(self, batch_size: int) -> Tensor:
        """Returns a zeroed state.
        Args:
            batch_size: batch size of the input
        Returns:
           Zeroed state tensor
        """
        device = self.weight_ih.get_analog_tile_devices()[0]
        return zeros(batch_size, self.hidden_size, device=device)

layer:
""" Analog RNN layers """

from typing import Any, List, Tuple, Type, Union
from torch import Tensor, stack, jit, cat
from torch.nn import ModuleList
from aihwkit.nn import AnalogSequential


class AnalogRNNLayer(AnalogSequential):
    """Analog RNN Layer.
    Args:
        cell: RNNCell type (e.g. AnalogLSTMCell)
        cell_args: arguments to RNNCell (e.g. input_size, hidden_size, rpu_configs)
    """
    # pylint: disable=abstract-method

    def __init__(self, cell: Type, *cell_args: Any):
        super().__init__()
        self.cell = cell(*cell_args)

    def get_zero_state(self, batch_size: int) -> Tensor:
        """Returns a zeroed state.
        Args:
            batch_size: batch size of the input
        Returns:
           Zeroed state tensor
        """
        return self.cell.get_zero_state(batch_size)

    def forward(
            self, input_: Tensor,
            state: Union[Tuple[Tensor, Tensor], Tensor]
    ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
        # pylint: disable=arguments-differ
        inputs = input_.unbind(0)
        outputs = jit.annotate(List[Tensor], [])
        for input_item in inputs:
            out, state = self.cell(input_item, state)
            outputs += [out]
        return stack(outputs), state

rnn:

import warnings
import math
from typing import Any, List, Optional, Tuple, Type, Callable
from torch import Tensor, jit
from torch.nn import Dropout, ModuleList, init

from aihwkit.nn import AnalogSequential
from aihwkit.nn.modules.rnn.layers import AnalogRNNLayer, AnalogBidirRNNLayer
from aihwkit.nn.modules.base import AnalogModuleBase, RPUConfigAlias


class ModularRNN(AnalogSequential):
    """Helper class to create a Modular RNN
    Args:
        num_layers: number of serially connected RNN layers
        layer: RNN layer type (e.g. AnalogLSTMLayer)
        dropout: dropout applied to output of all RNN layers except last
        first_layer_args: RNNCell type, input_size, hidden_size, rpu_config, etc.
        other_layer_args: RNNCell type, hidden_size, hidden_size, rpu_config, etc.
    """
    # pylint: disable=abstract-method

    # Necessary for iterating through self.layers and dropout support
    __constants__ = ['layers', 'num_layers']

    def __init__(
            self,
            num_layers: int,
            layer: Type,
            dropout: float,
            first_layer_args: Any,
            other_layer_args: Any):
        super().__init__()
        self.layers = self.init_stacked_analog_lstm(num_layers, layer, first_layer_args,
                                                    other_layer_args)

        # Introduce a Dropout layer on the outputs of each RNN layer except
        # the last layer.
        self.num_layers = num_layers
        if num_layers == 1 and dropout > 0:
            warnings.warn('dropout lstm adds dropout layers after all but last '
                          'recurrent layer, it expects num_layers greater than '
                          '1, but got num_layers = 1')
        self.dropout_layer = Dropout(dropout) if dropout else None

    @staticmethod
    def init_stacked_analog_lstm(
            num_layers: int,
            layer: Type,
            first_layer_args: Any,
            other_layer_args: Any
    ) -> ModuleList:
        """Construct a list of LSTMLayers over which to iterate.
        Args:
            num_layers: number of serially connected LSTM layers
            layer: RNN layer type (e.g. AnalogLSTMLayer)
            first_layer_args: RNNCell type, input_size, hidden_size, rpu_config, etc.
            other_layer_args: RNNCell type, hidden_size, hidden_size, rpu_config, etc.
        Returns:
            torch.nn.ModuleList, which is similar to a regular Python list,
            but where torch.nn.Module methods can be applied
        """
        layers = [layer(*first_layer_args)] \
            + [layer(*other_layer_args) for _ in range(num_layers - 1)]
        return ModuleList(layers)

